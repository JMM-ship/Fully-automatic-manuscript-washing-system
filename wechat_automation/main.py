#!/usr/bin/env python3
"""
å…¬ä¼—å·å†…å®¹è‡ªåŠ¨åŒ–ç³»ç»Ÿ - ä¸»ç¨‹åºå…¥å£
"""

import click
import asyncio
from pathlib import Path
import sys

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from crawler import WechatCrawler, read_urls_from_file
from converter import HtmlToMarkdownConverter
from classifier import ArticleClassifier
from extractor import MaterialExtractor
from image_tagger import ImageTagger
from creator import ContentCreator
from publisher import ContentPublisher

@click.group()
def cli():
    """å…¬ä¼—å·å†…å®¹è‡ªåŠ¨åŒ–ç³»ç»Ÿ MVP"""
    pass

@cli.command()
@click.argument('urls_file', type=click.Path(exists=True))
def crawl(urls_file):
    """æ‰¹é‡çˆ¬å–å…¬ä¼—å·æ–‡ç« 
    
    URLS_FILE: åŒ…å«æ–‡ç« é“¾æ¥çš„æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªé“¾æ¥
    """
    click.echo(f"è¯»å–é“¾æ¥æ–‡ä»¶: {urls_file}")
    
    # è¯»å–URLåˆ—è¡¨
    urls = read_urls_from_file(urls_file)
    click.echo(f"æ‰¾åˆ° {len(urls)} ä¸ªé“¾æ¥")
    
    # åˆ›å»ºçˆ¬è™«å¹¶è¿è¡Œ
    crawler = WechatCrawler()
    asyncio.run(crawler.crawl_articles(urls))
    
    click.echo("çˆ¬å–å®Œæˆï¼")

@cli.command()
def convert():
    """å°†HTMLæ–‡ç« è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    click.echo("å¼€å§‹è½¬æ¢æ–‡ç« ...")
    
    converter = HtmlToMarkdownConverter()
    converter.convert_all_articles()
    
    click.echo("è½¬æ¢å®Œæˆï¼")

@cli.command()
def classify():
    """å¯¹æ–‡ç« è¿›è¡Œæ™ºèƒ½åˆ†ç±»"""
    click.echo("å¼€å§‹åˆ†ç±»æ–‡ç« ...")
    
    classifier = ArticleClassifier()
    result = classifier.run()
    
    if result:
        click.echo("åˆ†ç±»å®Œæˆï¼")
    else:
        click.echo("åˆ†ç±»å¤±è´¥ï¼")

@cli.command()
def extract():
    """æå–æ–‡ç« çš„12ç»´åº¦ç´ æ"""
    click.echo("å¼€å§‹æå–ç´ æ...")
    
    extractor = MaterialExtractor()
    extractor.extract_all_themes()
    
    click.echo("ç´ ææå–å®Œæˆï¼")

@cli.command()
def tag_images():
    """ä¸ºå›¾ç‰‡è‡ªåŠ¨æ‰“æ ‡ç­¾"""
    click.echo("å¼€å§‹åˆ†æå›¾ç‰‡...")
    
    tagger = ImageTagger()
    tagger.tag_all_images()
    
    click.echo("å›¾ç‰‡æ ‡ç­¾å®Œæˆï¼")

@cli.command()
@click.argument('theme_name')
@click.option('--interactive', '-i', is_flag=True, help='äº¤äº’å¼åˆ›ä½œæ¨¡å¼')
@click.option('--batch', '-b', type=int, help='æ‰¹é‡åˆ›ä½œæ–‡ç« æ•°é‡')
def create(theme_name, interactive, batch):
    """åŸºäºç´ æåº“åˆ›ä½œæ–‡ç« 
    
    THEME_NAME: ä¸»é¢˜åç§°
    """
    creator = ContentCreator()
    
    if batch:
        # æ‰¹é‡åˆ›ä½œæ¨¡å¼
        creator.batch_create(theme_name, count=batch)
    elif interactive:
        # äº¤äº’å¼åˆ›ä½œæ¨¡å¼
        creator.interactive_create(theme_name)
    else:
        # é»˜è®¤åˆ›ä½œä¸€ç¯‡
        click.echo(f"ä¸ºä¸»é¢˜ {theme_name} åˆ›ä½œæ–‡ç« ...")
        article = creator.create_article(theme_name)
        
        if article:
            # è‡ªåŠ¨ä¼˜åŒ–
            article = creator.polish_article(article)
            
            # ä¿å­˜è‰ç¨¿
            draft_path = creator.save_draft(theme_name, article)
            click.echo(f"è‰ç¨¿å·²ä¿å­˜: {draft_path}")
        else:
            click.echo("åˆ›ä½œå¤±è´¥ï¼")

@cli.command()
@click.argument('theme_name')
@click.argument('draft_path', type=click.Path(exists=True))
def publish(theme_name, draft_path):
    """å‡†å¤‡å‘å¸ƒå†…å®¹
    
    THEME_NAME: ä¸»é¢˜åç§°
    DRAFT_PATH: è‰ç¨¿æ–‡ä»¶è·¯å¾„
    """
    click.echo(f"å‡†å¤‡å‘å¸ƒ: {draft_path}")
    
    publisher = ContentPublisher()
    output_path = publisher.prepare_for_publish(theme_name, draft_path)
    
    click.echo(f"å‘å¸ƒå†…å®¹å·²å‡†å¤‡: {output_path}")

@cli.command()
@click.argument('urls_file', type=click.Path(exists=True))
def process(urls_file):
    """å®Œæ•´å¤„ç†æµç¨‹ï¼šçˆ¬å–->è½¬æ¢->åˆ†ç±»->æå–ç´ æ
    
    URLS_FILE: åŒ…å«æ–‡ç« é“¾æ¥çš„æ–‡æœ¬æ–‡ä»¶
    """
    click.echo("å¼€å§‹å®Œæ•´å¤„ç†æµç¨‹...")
    
    # 1. çˆ¬å–
    click.echo("\n[1/5] çˆ¬å–æ–‡ç« ...")
    urls = read_urls_from_file(urls_file)
    crawler = WechatCrawler()
    asyncio.run(crawler.crawl_articles(urls))
    
    # 2. è½¬æ¢
    click.echo("\n[2/5] è½¬æ¢ä¸ºMarkdown...")
    converter = HtmlToMarkdownConverter()
    converter.convert_all_articles()
    
    # 3. åˆ†ç±»
    click.echo("\n[3/5] æ™ºèƒ½åˆ†ç±»...")
    classifier = ArticleClassifier()
    classifier.run()
    
    # 4. æå–ç´ æ
    click.echo("\n[4/5] æå–ç´ æ...")
    extractor = MaterialExtractor()
    extractor.extract_all_themes()
    
    # 5. å›¾ç‰‡æ ‡ç­¾
    click.echo("\n[5/5] åˆ†æå›¾ç‰‡...")
    tagger = ImageTagger()
    tagger.tag_all_images()
    
    click.echo("\nâœ… å®Œæ•´å¤„ç†æµç¨‹å®Œæˆï¼")
    click.echo("ä¸‹ä¸€æ­¥ï¼šä½¿ç”¨ 'python main.py create <ä¸»é¢˜å>' åˆ›ä½œæ–‡ç« ")

@cli.command()
def list_themes():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¸»é¢˜"""
    themes_path = Path("data/themes")
    
    if not themes_path.exists():
        click.echo("æœªæ‰¾åˆ°ä»»ä½•ä¸»é¢˜ï¼Œè¯·å…ˆè¿è¡Œåˆ†ç±»")
        return
    
    click.echo("å¯ç”¨ä¸»é¢˜ï¼š")
    
    for theme_dir in themes_path.iterdir():
        if theme_dir.is_dir() and theme_dir.name != "classification.json":
            # è¯»å–ä¸»é¢˜å…ƒæ•°æ®
            metadata_path = theme_dir / "metadata.json"
            if metadata_path.exists():
                import json
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                article_count = metadata.get('article_count', 0)
                description = metadata.get('description', '')
                
                click.echo(f"\nğŸ“ {theme_dir.name}")
                click.echo(f"   æ–‡ç« æ•°: {article_count}")
                if description:
                    click.echo(f"   æè¿°: {description}")

if __name__ == "__main__":
    cli()