#!/usr/bin/env python3
"""
å…¬ä¼—å·å†…å®¹è‡ªåŠ¨åŒ–ç³»ç»Ÿ - ä¸»ç¨‹åºå…¥å£
"""

import click
import asyncio
from pathlib import Path
import sys
import json
import os

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from crawler import WechatCrawler, read_urls_from_file
from converter import HtmlToMarkdownConverter
from classifier import ArticleClassifier
from extractor import MaterialExtractor
from image_tagger import ImageTagger
from creator import ContentCreator
from publisher import ContentPublisher

@click.group()
def cli():
    """å…¬ä¼—å·å†…å®¹è‡ªåŠ¨åŒ–ç³»ç»Ÿ MVP"""
    pass

@cli.command()
@click.argument('urls_file', type=click.Path(exists=True), required=False)
@click.option('--interactive', '-i', is_flag=True, help='äº¤äº’å¼è¾“å…¥URL')
def crawl(urls_file, interactive):
    """æ‰¹é‡çˆ¬å–å…¬ä¼—å·æ–‡ç« 
    
    URLS_FILE: åŒ…å«æ–‡ç« é“¾æ¥çš„æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªé“¾æ¥ï¼ˆå¯é€‰ï¼‰
    """
    urls = []
    
    if interactive or not urls_file:
        # äº¤äº’å¼è¾“å…¥æ¨¡å¼
        click.echo("è¯·è¾“å…¥å…¬ä¼—å·æ–‡ç« é“¾æ¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
        while True:
            url = click.prompt("URL", default="", show_default=False)
            if not url:
                break
            if url.startswith("http"):
                urls.append(url)
            else:
                click.echo("è¯·è¾“å…¥æœ‰æ•ˆçš„URLï¼ˆä»¥httpå¼€å¤´ï¼‰")
        
        if not urls:
            click.echo("æœªè¾“å…¥ä»»ä½•URLï¼Œé€€å‡º")
            return
            
        click.echo(f"\nå…±è¾“å…¥ {len(urls)} ä¸ªé“¾æ¥")
    else:
        # ä»æ–‡ä»¶è¯»å–
        click.echo(f"è¯»å–é“¾æ¥æ–‡ä»¶: {urls_file}")
        urls = read_urls_from_file(urls_file)
        click.echo(f"æ‰¾åˆ° {len(urls)} ä¸ªé“¾æ¥")
    
    # åˆ›å»ºçˆ¬è™«å¹¶è¿è¡Œ
    crawler = WechatCrawler()
    asyncio.run(crawler.crawl_articles(urls))
    
    click.echo("çˆ¬å–å®Œæˆï¼")

@cli.command()
def convert():
    """å°†HTMLæ–‡ç« è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    click.echo("å¼€å§‹è½¬æ¢æ–‡ç« ...")
    
    converter = HtmlToMarkdownConverter()
    converter.convert_all_articles()
    
    click.echo("è½¬æ¢å®Œæˆï¼")

@cli.command()
def classify():
    """å¯¹æ–‡ç« è¿›è¡Œæ™ºèƒ½åˆ†ç±»"""
    click.echo("å¼€å§‹åˆ†ç±»æ–‡ç« ...")
    
    classifier = ArticleClassifier()
    result = classifier.run()
    
    if result:
        click.echo("åˆ†ç±»å®Œæˆï¼")
    else:
        click.echo("åˆ†ç±»å¤±è´¥ï¼")

@cli.command()
def extract():
    """æå–æ–‡ç« çš„12ç»´åº¦ç´ æ"""
    click.echo("å¼€å§‹æå–ç´ æ...")
    
    extractor = MaterialExtractor()
    extractor.extract_all_themes()
    
    click.echo("ç´ ææå–å®Œæˆï¼")

@cli.command()
def tag_images():
    """ä¸ºå›¾ç‰‡è‡ªåŠ¨æ‰“æ ‡ç­¾"""
    click.echo("å¼€å§‹åˆ†æå›¾ç‰‡...")
    
    tagger = ImageTagger()
    tagger.tag_all_images()
    
    click.echo("å›¾ç‰‡æ ‡ç­¾å®Œæˆï¼")

@cli.command()
@click.argument('theme_name')
@click.option('--interactive', '-i', is_flag=True, help='äº¤äº’å¼åˆ›ä½œæ¨¡å¼')
@click.option('--batch', '-b', type=int, help='æ‰¹é‡åˆ›ä½œæ–‡ç« æ•°é‡')
def create(theme_name, interactive, batch):
    """åŸºäºç´ æåº“åˆ›ä½œæ–‡ç« 
    
    THEME_NAME: ä¸»é¢˜åç§°
    """
    creator = ContentCreator()
    
    if batch:
        # æ‰¹é‡åˆ›ä½œæ¨¡å¼
        creator.batch_create(theme_name, count=batch)
    elif interactive:
        # äº¤äº’å¼åˆ›ä½œæ¨¡å¼
        creator.interactive_create(theme_name)
    else:
        # é»˜è®¤åˆ›ä½œä¸€ç¯‡
        click.echo(f"ä¸ºä¸»é¢˜ {theme_name} åˆ›ä½œæ–‡ç« ...")
        article = creator.create_article(theme_name)
        
        if article:
            # è‡ªåŠ¨ä¼˜åŒ–
            article = creator.polish_article(article)
            
            # ä¿å­˜è‰ç¨¿
            draft_path = creator.save_draft(theme_name, article)
            click.echo(f"è‰ç¨¿å·²ä¿å­˜: {draft_path}")
        else:
            click.echo("åˆ›ä½œå¤±è´¥ï¼")

@cli.command()
@click.argument('theme_name')
@click.argument('draft_path', type=click.Path(exists=True))
def publish(theme_name, draft_path):
    """å‡†å¤‡å‘å¸ƒå†…å®¹
    
    THEME_NAME: ä¸»é¢˜åç§°
    DRAFT_PATH: è‰ç¨¿æ–‡ä»¶è·¯å¾„
    """
    click.echo(f"å‡†å¤‡å‘å¸ƒ: {draft_path}")
    
    publisher = ContentPublisher()
    output_path = publisher.prepare_for_publish(theme_name, draft_path)
    
    click.echo(f"å‘å¸ƒå†…å®¹å·²å‡†å¤‡: {output_path}")

@cli.command()
@click.argument('urls_file', type=click.Path(exists=True), required=False)
@click.option('--interactive', '-i', is_flag=True, help='äº¤äº’å¼è¾“å…¥URL')
def process(urls_file, interactive):
    """å®Œæ•´å¤„ç†æµç¨‹ï¼šçˆ¬å–->è½¬æ¢->åˆ†ç±»->æå–ç´ æ
    
    URLS_FILE: åŒ…å«æ–‡ç« é“¾æ¥çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    """
    click.echo("å¼€å§‹å®Œæ•´å¤„ç†æµç¨‹...")
    
    # 1. çˆ¬å–
    click.echo("\n[1/5] çˆ¬å–æ–‡ç« ...")
    urls = []
    
    if interactive or not urls_file:
        # äº¤äº’å¼è¾“å…¥æ¨¡å¼
        click.echo("è¯·è¾“å…¥å…¬ä¼—å·æ–‡ç« é“¾æ¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
        while True:
            url = click.prompt("URL", default="", show_default=False)
            if not url:
                break
            if url.startswith("http"):
                urls.append(url)
            else:
                click.echo("è¯·è¾“å…¥æœ‰æ•ˆçš„URLï¼ˆä»¥httpå¼€å¤´ï¼‰")
        
        if not urls:
            click.echo("æœªè¾“å…¥ä»»ä½•URLï¼Œé€€å‡º")
            return
            
        click.echo(f"\nå…±è¾“å…¥ {len(urls)} ä¸ªé“¾æ¥")
    else:
        # ä»æ–‡ä»¶è¯»å–
        urls = read_urls_from_file(urls_file)
        click.echo(f"ä»æ–‡ä»¶è¯»å– {len(urls)} ä¸ªé“¾æ¥")
    
    crawler = WechatCrawler()
    asyncio.run(crawler.crawl_articles(urls))
    
    # 2. è½¬æ¢
    click.echo("\n[2/5] è½¬æ¢ä¸ºMarkdown...")
    converter = HtmlToMarkdownConverter()
    converter.convert_all_articles()
    
    # 3. åˆ†ç±»
    click.echo("\n[3/5] æ™ºèƒ½åˆ†ç±»...")
    classifier = ArticleClassifier()
    classifier.run()
    
    # 4. æå–ç´ æ
    click.echo("\n[4/5] æå–ç´ æ...")
    extractor = MaterialExtractor()
    extractor.extract_all_themes()
    
    # 5. å›¾ç‰‡æ ‡ç­¾
    click.echo("\n[5/5] åˆ†æå›¾ç‰‡...")
    tagger = ImageTagger()
    tagger.tag_all_images()
    
    click.echo("\nâœ… å®Œæ•´å¤„ç†æµç¨‹å®Œæˆï¼")
    click.echo("ä¸‹ä¸€æ­¥ï¼šä½¿ç”¨ 'python main.py create <ä¸»é¢˜å>' åˆ›ä½œæ–‡ç« ")

@cli.command()
@click.argument('urls_file', type=click.Path(exists=True), required=False)
@click.option('--interactive', '-i', is_flag=True, help='äº¤äº’å¼è¾“å…¥URL')
def process_with_review(urls_file, interactive):
    """å¸¦äººå·¥å®¡æ ¸çš„å¤„ç†æµç¨‹ï¼šæ¯ä¸ªé˜¶æ®µå®Œæˆåäººå·¥ç¡®è®¤
    
    URLS_FILE: åŒ…å«æ–‡ç« é“¾æ¥çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    """
    click.echo("å¼€å§‹å¸¦å®¡æ ¸çš„å¤„ç†æµç¨‹...")
    click.echo("æ¯ä¸ªé˜¶æ®µå®Œæˆåå°†å±•ç¤ºç»“æœï¼Œç”±æ‚¨å†³å®šæ˜¯å¦ç»§ç»­ã€é‡åšæˆ–è·³è¿‡")
    
    # 1. çˆ¬å–é˜¶æ®µ
    click.echo("\n[1/5] çˆ¬å–æ–‡ç« ...")
    urls = []
    
    if interactive or not urls_file:
        # äº¤äº’å¼è¾“å…¥æ¨¡å¼
        click.echo("è¯·è¾“å…¥å…¬ä¼—å·æ–‡ç« é“¾æ¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
        while True:
            url = click.prompt("URL", default="", show_default=False)
            if not url:
                break
            if url.startswith("http"):
                urls.append(url)
            else:
                click.echo("è¯·è¾“å…¥æœ‰æ•ˆçš„URLï¼ˆä»¥httpå¼€å¤´ï¼‰")
        
        if not urls:
            click.echo("æœªè¾“å…¥ä»»ä½•URLï¼Œé€€å‡º")
            return
            
        click.echo(f"\nå…±è¾“å…¥ {len(urls)} ä¸ªé“¾æ¥")
    else:
        # ä»æ–‡ä»¶è¯»å–
        urls = read_urls_from_file(urls_file)
        click.echo(f"ä»æ–‡ä»¶è¯»å– {len(urls)} ä¸ªé“¾æ¥")
    
    # æ‰§è¡Œçˆ¬å–
    crawler = WechatCrawler()
    results = asyncio.run(crawler.crawl_articles(urls))
    
    # æ˜¾ç¤ºçˆ¬å–ç»“æœ
    raw_articles_path = Path("data/raw_articles")
    if results:
        click.echo(f"\nâœ… çˆ¬å–æˆåŠŸï¼å…±è·å– {len(results)} ç¯‡æ–‡ç« ")
        click.echo("æ–‡ç« åˆ—è¡¨ï¼š")
        for result in results[:5]:  # åªæ˜¾ç¤ºå‰5ç¯‡
            click.echo(f"  - {result['title']} ({result['id']})")
        if len(results) > 5:
            click.echo(f"  ... è¿˜æœ‰ {len(results) - 5} ç¯‡")
    else:
        click.echo("\nâŒ çˆ¬å–å¤±è´¥ï¼æœªè·å–åˆ°ä»»ä½•æ–‡ç« ")
        click.echo("è¯·æ£€æŸ¥ï¼š")
        click.echo("  1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        click.echo("  2. URLæ˜¯å¦æœ‰æ•ˆ")
        click.echo("  3. æŸ¥çœ‹ä¸Šæ–¹çš„é”™è¯¯ä¿¡æ¯")
    
    # å®¡æ ¸çˆ¬å–ç»“æœ
    while True:
        action = click.prompt("\nè¯·é€‰æ‹©ï¼š[c]ç»§ç»­ä¸‹ä¸€æ­¥ / [r]é‡æ–°çˆ¬å– / [s]è·³è¿‡åç»­æ­¥éª¤ / [q]é€€å‡º", 
                             type=click.Choice(['c', 'r', 's', 'q']), 
                             default='c')
        
        if action == 'c':
            if not results:
                click.echo("è­¦å‘Šï¼šæ²¡æœ‰æˆåŠŸçˆ¬å–çš„æ–‡ç« ï¼Œåç»­æ­¥éª¤å¯èƒ½æ— æ³•æ­£å¸¸æ‰§è¡Œ")
                if not click.confirm("ç¡®å®šè¦ç»§ç»­å—ï¼Ÿ"):
                    continue
            break
        elif action == 'r':
            click.echo("é‡æ–°æ‰§è¡Œçˆ¬å–...")
            results = asyncio.run(crawler.crawl_articles(urls))
            
            # é‡æ–°æ˜¾ç¤ºç»“æœ
            if results:
                click.echo(f"\nâœ… çˆ¬å–æˆåŠŸï¼å…±è·å– {len(results)} ç¯‡æ–‡ç« ")
                click.echo("æ–‡ç« åˆ—è¡¨ï¼š")
                for result in results[:5]:
                    click.echo(f"  - {result['title']} ({result['id']})")
                if len(results) > 5:
                    click.echo(f"  ... è¿˜æœ‰ {len(results) - 5} ç¯‡")
            else:
                click.echo("\nâŒ çˆ¬å–å¤±è´¥ï¼æœªè·å–åˆ°ä»»ä½•æ–‡ç« ")
            continue
        elif action == 's':
            click.echo("è·³è¿‡åç»­æ­¥éª¤")
            return
        elif action == 'q':
            click.echo("é€€å‡ºç¨‹åº")
            return
    
    # 2. è½¬æ¢é˜¶æ®µ
    click.echo("\n[2/5] è½¬æ¢ä¸ºMarkdown...")
    converter = HtmlToMarkdownConverter()
    converter.convert_all_articles()
    
    # æ˜¾ç¤ºè½¬æ¢ç»“æœ
    markdown_path = Path("data/markdown")
    if markdown_path.exists():
        md_files = list(markdown_path.glob("*.md"))
        click.echo(f"\nâœ… è½¬æ¢å®Œæˆï¼å…±ç”Ÿæˆ {len(md_files)} ä¸ªMarkdownæ–‡ä»¶")
        
        # é¢„è§ˆç¬¬ä¸€ä¸ªæ–‡ä»¶çš„éƒ¨åˆ†å†…å®¹
        if md_files:
            sample_file = md_files[0]
            with open(sample_file, 'r', encoding='utf-8') as f:
                content = f.read()
                preview = content[:300] + "..." if len(content) > 300 else content
            click.echo(f"\né¢„è§ˆ {sample_file.name}:")
            click.echo("-" * 50)
            click.echo(preview)
            click.echo("-" * 50)
    
    # å®¡æ ¸è½¬æ¢ç»“æœ
    while True:
        action = click.prompt("\nè¯·é€‰æ‹©ï¼š[c]ç»§ç»­ä¸‹ä¸€æ­¥ / [r]é‡æ–°è½¬æ¢ / [v]æŸ¥çœ‹æ›´å¤šæ–‡ä»¶ / [s]è·³è¿‡åç»­æ­¥éª¤ / [q]é€€å‡º", 
                             type=click.Choice(['c', 'r', 'v', 's', 'q']), 
                             default='c')
        
        if action == 'c':
            break
        elif action == 'r':
            click.echo("é‡æ–°æ‰§è¡Œè½¬æ¢...")
            converter.convert_all_articles()
            continue
        elif action == 'v':
            # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶è®©ç”¨æˆ·é€‰æ‹©æŸ¥çœ‹
            for i, md_file in enumerate(md_files[:10]):
                click.echo(f"{i+1}. {md_file.name}")
            if len(md_files) > 10:
                click.echo("... (åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶)")
            
            file_num = click.prompt("è¯·è¾“å…¥è¦æŸ¥çœ‹çš„æ–‡ä»¶ç¼–å·", type=int)
            if 1 <= file_num <= min(len(md_files), 10):
                selected_file = md_files[file_num - 1]
                with open(selected_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    preview = content[:500] + "..." if len(content) > 500 else content
                click.echo(f"\né¢„è§ˆ {selected_file.name}:")
                click.echo("-" * 50)
                click.echo(preview)
                click.echo("-" * 50)
        elif action == 's':
            click.echo("è·³è¿‡åç»­æ­¥éª¤")
            return
        elif action == 'q':
            click.echo("é€€å‡ºç¨‹åº")
            return
    
    # 3. åˆ†ç±»é˜¶æ®µ
    click.echo("\n[3/5] æ™ºèƒ½åˆ†ç±»...")
    classifier = ArticleClassifier()
    result = classifier.run()
    
    # æ˜¾ç¤ºåˆ†ç±»ç»“æœ
    if result:
        themes_path = Path("data/themes")
        classification_file = themes_path / "classification.json"
        
        if classification_file.exists():
            with open(classification_file, 'r', encoding='utf-8') as f:
                classification = json.load(f)
            
            click.echo(f"\nâœ… åˆ†ç±»å®Œæˆï¼å…±è¯†åˆ«å‡º {len(classification['themes'])} ä¸ªä¸»é¢˜")
            click.echo("\nä¸»é¢˜åˆ†ç±»ç»“æœï¼š")
            
            for theme in classification['themes']:
                click.echo(f"\nğŸ“ {theme.get('theme_name', 'æœªå‘½åä¸»é¢˜')}")
                click.echo(f"   æè¿°ï¼š{theme.get('description', 'æ— æè¿°')}")
                click.echo(f"   æ–‡ç« æ•°ï¼š{len(theme.get('articles', []))}")
                click.echo(f"   æ–‡ç« åˆ—è¡¨ï¼š")
                articles = theme.get('articles', [])
                for article in articles[:3]:  # åªæ˜¾ç¤ºå‰3ç¯‡
                    click.echo(f"     - {article}")
                if len(articles) > 3:
                    click.echo(f"     ... è¿˜æœ‰ {len(articles) - 3} ç¯‡")
    
    # å®¡æ ¸åˆ†ç±»ç»“æœ
    while True:
        action = click.prompt("\nè¯·é€‰æ‹©ï¼š[c]ç»§ç»­ä¸‹ä¸€æ­¥ / [r]é‡æ–°åˆ†ç±» / [e]ç¼–è¾‘åˆ†ç±» / [s]è·³è¿‡åç»­æ­¥éª¤ / [q]é€€å‡º", 
                             type=click.Choice(['c', 'r', 'e', 's', 'q']), 
                             default='c')
        
        if action == 'c':
            break
        elif action == 'r':
            click.echo("é‡æ–°æ‰§è¡Œåˆ†ç±»...")
            result = classifier.run()
            continue
        elif action == 'e':
            click.echo("ç¼–è¾‘åˆ†ç±»åŠŸèƒ½æš‚æœªå®ç°ï¼Œè¯·æ‰‹åŠ¨ä¿®æ”¹ data/themes/classification.json æ–‡ä»¶")
        elif action == 's':
            click.echo("è·³è¿‡åç»­æ­¥éª¤")
            return
        elif action == 'q':
            click.echo("é€€å‡ºç¨‹åº")
            return
    
    # 4. æå–ç´ æé˜¶æ®µ
    click.echo("\n[4/5] æå–ç´ æ...")
    extractor = MaterialExtractor()
    extractor.extract_all_themes()
    
    # æ˜¾ç¤ºæå–ç»“æœ
    themes_path = Path("data/themes")
    theme_dirs = [d for d in themes_path.iterdir() if d.is_dir() and d.name != "classification.json"]
    
    click.echo(f"\nâœ… ç´ ææå–å®Œæˆï¼")
    for theme_dir in theme_dirs:
        material_path = theme_dir / "ç´ æåº“"
        if material_path.exists():
            material_files = list(material_path.glob("*.json"))
            click.echo(f"\nğŸ“ {theme_dir.name}: æå–äº† {len(material_files)} ä¸ªç´ ææ–‡ä»¶")
            
            # é¢„è§ˆç¬¬ä¸€ä¸ªç´ ææ–‡ä»¶
            if material_files:
                with open(material_files[0], 'r', encoding='utf-8') as f:
                    material = json.load(f)
                click.echo(f"   é¢„è§ˆç´ æç»´åº¦ï¼š")
                for key in list(material.keys())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªç»´åº¦
                    click.echo(f"   - {key}")
    
    # å®¡æ ¸æå–ç»“æœ
    while True:
        action = click.prompt("\nè¯·é€‰æ‹©ï¼š[c]ç»§ç»­ä¸‹ä¸€æ­¥ / [r]é‡æ–°æå– / [v]æŸ¥çœ‹ç´ æè¯¦æƒ… / [s]è·³è¿‡åç»­æ­¥éª¤ / [q]é€€å‡º", 
                             type=click.Choice(['c', 'r', 'v', 's', 'q']), 
                             default='c')
        
        if action == 'c':
            break
        elif action == 'r':
            click.echo("é‡æ–°æ‰§è¡Œç´ ææå–...")
            extractor.extract_all_themes()
            continue
        elif action == 'v':
            # é€‰æ‹©ä¸»é¢˜æŸ¥çœ‹
            for i, theme_dir in enumerate(theme_dirs):
                click.echo(f"{i+1}. {theme_dir.name}")
            
            theme_num = click.prompt("è¯·è¾“å…¥è¦æŸ¥çœ‹çš„ä¸»é¢˜ç¼–å·", type=int)
            if 1 <= theme_num <= len(theme_dirs):
                selected_theme = theme_dirs[theme_num - 1]
                material_path = selected_theme / "ç´ æåº“"
                material_files = list(material_path.glob("*.json"))
                
                if material_files:
                    with open(material_files[0], 'r', encoding='utf-8') as f:
                        material = json.load(f)
                    click.echo(f"\n{selected_theme.name} çš„ç´ æç¤ºä¾‹ï¼š")
                    click.echo("-" * 50)
                    for key, value in list(material.items())[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªç»´åº¦
                        click.echo(f"{key}:")
                        if isinstance(value, list):
                            for item in value[:2]:  # æ¯ä¸ªç»´åº¦åªæ˜¾ç¤ºå‰2é¡¹
                                click.echo(f"  - {item}")
                        else:
                            click.echo(f"  {value}")
                    click.echo("-" * 50)
        elif action == 's':
            click.echo("è·³è¿‡åç»­æ­¥éª¤")
            return
        elif action == 'q':
            click.echo("é€€å‡ºç¨‹åº")
            return
    
    # 5. å›¾ç‰‡æ ‡ç­¾é˜¶æ®µ
    click.echo("\n[5/5] åˆ†æå›¾ç‰‡...")
    tagger = ImageTagger()
    tagger.tag_all_images()
    
    # æ˜¾ç¤ºæ ‡ç­¾ç»“æœ
    images_path = Path("data/images")
    if images_path.exists():
        image_files = list(images_path.glob("*.jpg")) + list(images_path.glob("*.png"))
        tagged_count = sum(1 for img in image_files if (img.parent / f"{img.stem}_tags.json").exists())
        
        click.echo(f"\nâœ… å›¾ç‰‡åˆ†æå®Œæˆï¼")
        click.echo(f"å…±æœ‰ {len(image_files)} å¼ å›¾ç‰‡ï¼Œå·²æ ‡è®° {tagged_count} å¼ ")
        
        # é¢„è§ˆä¸€ä¸ªæ ‡ç­¾æ–‡ä»¶
        for img in image_files:
            tag_file = img.parent / f"{img.stem}_tags.json"
            if tag_file.exists():
                with open(tag_file, 'r', encoding='utf-8') as f:
                    tags = json.load(f)
                click.echo(f"\nå›¾ç‰‡ {img.name} çš„æ ‡ç­¾ï¼š")
                click.echo(f"  {tags}")
                break
    
    # æœ€ç»ˆå®¡æ ¸
    action = click.prompt("\næ‰€æœ‰é˜¶æ®µå·²å®Œæˆï¼æ˜¯å¦æŸ¥çœ‹æœ€ç»ˆç»Ÿè®¡ï¼Ÿ[y/n]", 
                         type=click.Choice(['y', 'n']), 
                         default='y')
    
    if action == 'y':
        click.echo("\nğŸ“Š å¤„ç†ç»Ÿè®¡ï¼š")
        click.echo(f"- çˆ¬å–æ–‡ç« æ•°ï¼š{len(articles) if 'articles' in locals() else 0}")
        click.echo(f"- è½¬æ¢æ–‡æ¡£æ•°ï¼š{len(md_files) if 'md_files' in locals() else 0}")
        click.echo(f"- è¯†åˆ«ä¸»é¢˜æ•°ï¼š{len(classification['themes']) if 'classification' in locals() else 0}")
        click.echo(f"- æå–ç´ ææ•°ï¼š{sum(len(list((t / 'ç´ æåº“').glob('*.json'))) for t in theme_dirs if (t / 'ç´ æåº“').exists()) if 'theme_dirs' in locals() else 0}")
        click.echo(f"- æ ‡è®°å›¾ç‰‡æ•°ï¼š{tagged_count if 'tagged_count' in locals() else 0}")
    
    click.echo("\nâœ… å¸¦å®¡æ ¸çš„å¤„ç†æµç¨‹å®Œæˆï¼")
    click.echo("ä¸‹ä¸€æ­¥ï¼šä½¿ç”¨ 'python main.py create <ä¸»é¢˜å>' åˆ›ä½œæ–‡ç« ")

@cli.command()
def list_themes():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¸»é¢˜"""
    themes_path = Path("data/themes")
    
    if not themes_path.exists():
        click.echo("æœªæ‰¾åˆ°ä»»ä½•ä¸»é¢˜ï¼Œè¯·å…ˆè¿è¡Œåˆ†ç±»")
        return
    
    click.echo("å¯ç”¨ä¸»é¢˜ï¼š")
    
    for theme_dir in themes_path.iterdir():
        if theme_dir.is_dir() and theme_dir.name != "classification.json":
            # è¯»å–ä¸»é¢˜å…ƒæ•°æ®
            metadata_path = theme_dir / "metadata.json"
            if metadata_path.exists():
                import json
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                article_count = metadata.get('article_count', 0)
                description = metadata.get('description', '')
                
                click.echo(f"\nğŸ“ {theme_dir.name}")
                click.echo(f"   æ–‡ç« æ•°: {article_count}")
                if description:
                    click.echo(f"   æè¿°: {description}")

if __name__ == "__main__":
    cli()